{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd3363c-0e09-47f7-8953-9f2f887f624d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformer Based Batch Inference\n",
    "\n",
    "This notebook aims to show how you can run batch inference using Spark's distributed capabilites, with a multi-machine multi-gpu setup. The code is designed to fit the model used into a single GPU, therefore some adjustment mightbe needed for using a larger model or a GPU with smaller memory.\n",
    "\n",
    "Environment for this notebook:\n",
    "- Runtime: 14.0 GPU ML Runtime\n",
    "- Instance: `N24ads_A100_V4` on Azure. 3 A100 GPUs (1 Driver + 2 Worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632a9a3f-f2a3-4415-a9aa-b8780eafed00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install & Upgrade Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96943394-f803-46e9-8cee-9336fe79a47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers\n",
    "!pip install -q --upgrade accelerate\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3d7111-d60c-44ac-940e-e9127a4a3856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GPU Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7fa476-5bf5-446d-a97d-502ddc2b26bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 18 21:03:50 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100 80G...  Off  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   37C    P0    45W / 300W |      0MiB / 80994MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9033c85a-47bd-4cdf-a789-9f46ab5740a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The model name & tokenizer name below should be adjustable to most of the other models existing in the hugging face world. The code is designed to fit the entire model into a single GPU, so futher tweaking might be required if a large model or a GPU with small memory is used.\n",
    "\n",
    "It would also make sense to change the prompt if there is a change to the model. This one is specifically designed for the LLAMA V2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fd51cb-d946-4d34-8635-f5470d1cfd35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Params\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MODEL_REVISION_ID = \"08751db2aca9bf2f7f80d2e516117a53d7450235\"\n",
    "MAX_NEW_TOKENS = 300\n",
    "MIN_LENGTH = 0\n",
    "REPETITION_PENALTY = 1.2\n",
    "TEMPERATURE = 0.1\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "DO_SAMPLE = True\n",
    "USE_CACHE = True\n",
    "\n",
    "# Tokenizer Params\n",
    "TOKENIZER_NAME = MODEL_NAME\n",
    "TOKENIZER_REVISION_ID = MODEL_REVISION_ID\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# Run Params (How many articles to use)\n",
    "MAX_EXAMPLES = 10000\n",
    "\n",
    "# Storage Params\n",
    "STORAGE_PATH = \"/dbfs/llm-examples\"\n",
    "\n",
    "# Instruction\n",
    "INSTRUCTION = \"\"\"Please provide a concise summary for the following article: {text}\"\"\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT_TEMPLATE = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a direct and honest assistant. Please provide concise and factual answers and just the answers.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{INSTRUCTION}\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c61ab44-949e-4ee6-9dc4-2869a49c30d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storage Operations\n",
    "\n",
    "Makes sure that the directory is cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6921db-f3bf-4c43-9571-1728a231122c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Remove existing files\n",
    "shutil.rmtree(STORAGE_PATH, ignore_errors=True)\n",
    "\n",
    "# Build the new directory\n",
    "os.makedirs(STORAGE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4be6d00-bd7f-463f-b4dd-ed8672b29ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Huggingface Login\n",
    "\n",
    "This step can be skipped if your model doesn't require a login. LLAMA V2 does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a57d541-4da7-428a-ac83-b7f7d511434a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d78ae8ed4048c7bed0c8c07b62c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login the huggingface\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973d0f78-5c75-4aa4-b892-9a4b8d36570a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Retrieve Articles Data\n",
    "\n",
    "CNN & Daily Mail articles data set from Hugging Face Datasets is retrieved and combined to make a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4ee291-7387-45f8-b5e3-c070870ff7da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:109: UserWarning: The dataset would be saved to both local disk and DBFS for better performance.\n  warnings.warn(\"The dataset would be saved to both local disk and DBFS for better performance.\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from datasets.utils import logging as dataset_logging, disable_progress_bar\n",
    "from pyspark.sql import functions as SF\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Disable verbose loggers\n",
    "dataset_logging.set_verbosity_error()\n",
    "disable_progress_bar()\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_dataset(\n",
    "    path=\"cnn_dailymail\", name=\"3.0.0\", cache_dir=f\"{STORAGE_PATH}/hf\"\n",
    ")\n",
    "\n",
    "# Create spark dataframes\n",
    "train_df = spark.createDataFrame(data=dataset[\"train\"].to_pandas())\n",
    "val_df = spark.createDataFrame(data=dataset[\"validation\"].to_pandas())\n",
    "test_df = spark.createDataFrame(data=dataset[\"test\"].to_pandas())\n",
    "\n",
    "# Union for all data\n",
    "articles_df = train_df.union(val_df).union(test_df)\n",
    "articles_df = articles_df.repartition(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ffe435-7e6a-4127-a5eb-d7b6c29a50e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sample Data\n",
    "\n",
    "Deterministic sampling through ID hexing for consistent results & benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b64b75b-f05d-4785-9654-fe5b9cac703e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "import hashlib\n",
    "\n",
    "# Build function for creating a random column\n",
    "@SF.udf(\"string\")\n",
    "def generate_hex_from_string(input_string: str) -> str:\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(input_string.encode(\"utf-8\"))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "\n",
    "# Generate random string\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"random_string\", generate_hex_from_string(SF.col(\"id\"))\n",
    ")\n",
    "\n",
    "# Order by randomness and limit dataframe size\n",
    "articles_df = articles_df.orderBy(SF.col(\"random_string\")).limit(MAX_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6d5212-466b-4d68-b2be-9bc29e1dfedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Prompts\n",
    "\n",
    "Prompts are applied with the article text to generate prepared instructions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d8ca41-0af4-41e8-b954-44218c2da0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build function for generating instructions\n",
    "@SF.udf(\"string\")\n",
    "def generate_instructions(article):\n",
    "    return PROMPT_TEMPLATE.format(text=article)\n",
    "\n",
    "\n",
    "# Generate instructions\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction\", generate_instructions(SF.col(\"article\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62ed7e9-7f9f-4659-a659-6d51a5c65e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execute Data Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07bd080-a45e-45dd-85f1-87145e4da39e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Cache for performance\n",
    "articles_df.cache()\n",
    "\n",
    "# Trigger with action\n",
    "articles_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "print(f\"Number of Examples: {articles_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057d87e4-9e0a-4dab-8672-11f51fd5fb69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download Model & Tokenizer\n",
    "\n",
    "Downloading the model and the tokizer helps when it comes to loading the model faster during the multi machine inference step.\n",
    "\n",
    "If the model and the tokenizer are in the same repository, only one download will occur. In some cases, for example for Falcon-7B, they can be different. In that case, the code downloads both to the location specified in params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e326895c-391c-4c1b-8fad-bf3fa9676cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from huggingface_hub.utils import (\n",
    "    disable_progress_bars as hfhub_disable_progress_bar,\n",
    "    logging as hf_logging,   \n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Turn Off Info Logging for Transfomers\n",
    "hf_logging.set_verbosity_error()\n",
    "hfhub_disable_progress_bar()\n",
    "\n",
    "# Download the model \n",
    "local_model_path = f\"{STORAGE_PATH}/model/\"\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "model_download = snapshot_download(\n",
    "    repo_id=MODEL_NAME,\n",
    "    revision=MODEL_REVISION_ID,\n",
    "    local_dir=local_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    ignore_patterns=\"*.safetensors*\", # This argument is specific to LLAMA. Other models might not need it.\n",
    "    max_workers=48 # Boosts download speed\n",
    ")\n",
    "\n",
    "# Download the tokenizer\n",
    "if MODEL_NAME == TOKENIZER_NAME:\n",
    "    local_tokenizer_path = local_model_path\n",
    "else:\n",
    "    local_tokenizer_path = f\"{STORAGE_PATH}/tokenizer/\"\n",
    "    os.makedirs(local_tokenizer_path, exist_ok=True)\n",
    "    tokenizer_download = snapshot_download(\n",
    "        repo_id=TOKENIZER_NAME,\n",
    "        local_dir=local_tokenizer_path,\n",
    "        local_dir_use_symlinks=False,\n",
    "        max_workers=48\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea24216-9024-4ad1-b74f-629d72a38a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Model & Tokenizer\n",
    "\n",
    "Model and Tokenizer are loaded for the downloaded directory for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3c804b-c242-40e6-b2e9-3653f6b46152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac88d9f7872481f82b915061a33e300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6992b99cb944082ac25798a2cc6fa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Params\n",
    "random_seed = 42\n",
    "\n",
    "# Random seed set\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load model to GPU\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1b16cb-d267-4b35-9f13-3fed6a2e4f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Test Run on Single Example\n",
    "\n",
    "Batch Generate functions takes a list of prompts, and returns a list of ouputs (generated_text). Generation parameters such a temperature and top_p are set within the function.\n",
    "\n",
    "Even though this example shows how to do a few examples, this function will be used during distributed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d49fd56-5c18-4c55-95eb-a5b7768cee69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\nYou are a direct and honest assistant. Please provide concise and factual answers and just the answers.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\nPlease provide a concise summary for the following article: Carl Froch will have to wait for his big Las Vegas farewell against Julio Cesar Chavez Jnr after the world champion injured his elbow in training last week. Almost all the details had been agreed on a March 28 fight that would have brought an end to Froch’s stellar career. But Sportsmail can reveal that an injury three weeks into his training camp has forced Froch to temporarily scrap his plans. His promoter Eddie Hearn is already in talks to rearrange the fight with dates considered in late April and early June. Carl Froch (right) will have to wait for his chance to realise his dream of fighting in Las Vegas . Froch has not fought since he knocked out George Groves in front of 80,000 people at Wembley last year . Hearn said: ‘Carl is devastated but given the scale of the fight and the fact it may be his last, he didn’t want to take any half measures by fighting injured. ‘We were very close to concluding negotiations so as you can imagine it was heartbreaking for Carl to make the call. But we are already looking to reschedule the fight. ‘Everyone knows Carl’s desire to fight in Vegas so we are doing what we can to make it happen. I fully believe it will happen, but he rightly does not want to go into what might be his last fight with an injury.’ Rumours of Froch’s retirement spread on social media on Sunday evening after Froch tweeted a picture of a pair of gloves hanging up. A deal for Froch to fight Julio Cesar Chavez Jnr (right) had been close before he suffered his injury . Chavez has not fought since winning his rematch with Brian Vera in Texas last March . Hearn insisted the WBA and IBF super-middleweight champion’s priority remains one last fight, having last fought in his remarkable Wembley rematch win against George Groves in May. Froch had reiterated his hopes for a Vegas fight earlier this month, saying: ‘Dare I say it, but we’re nearly there with the Chavez fight in Las Vegas which we’ve long talked about.' But he said on Monday: 'I'm disappointed to have to let this opportunity go but at this stage in my career I have to make every fight count. 'I will get some treatment and then we will look to make this fight or another big fight in the early summer.' Julio Cesar Chavez Jnr posted this on Instagram after hearing abour of Carl Froch's injury .\n[/INST]\nHere is a concise summary of the article:\nCarl Froch will have to postpone his highly anticipated boxing match against Julio Cesar Chavez Jr due to an elbow injury sustained during training. The two fighters had been close to finalizing a deal for the March 28th fight, which was expected to be Froch's last bout before retiring. However, Froch's promoter Eddie Hearn revealed that the injury occurred three weeks into Froch's training camp and has forced him to put the fight on hold. Negotiations are currently underway to reschedule the fight, with potential new dates in late April and early June being considered. Despite the setback, Froch remains determined to continue his boxing career and ensure that his final fight is a memorable one.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(2).collect()]\n",
    "\n",
    "# Define Inference Flow\n",
    "@torch.inference_mode()\n",
    "def batch_generate(batch_prompts, tokenizer=tokenizer, model=model):\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "        max_length=MAX_TOKENS\n",
    "    )\n",
    "    batch = {k: v.to(\"cuda:0\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE,\n",
    "            min_length=MIN_LENGTH,\n",
    "            use_cache=USE_CACHE,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Check out one example\n",
    "print(batch_generate(sample_instructions[:1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646f6112-3d8c-4f12-916a-014627a58f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Batch Test\n",
    "\n",
    "Optimal batch size changes depending on the GPU used. The GPU used in during this test has 80 GB of GPU memory, so going higher makes sense, however smaller machine like the A10s usually do better with smaller batch sizes.\n",
    "\n",
    "The code compares multiple batch sizes, and interation stops when out of memory error is raised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67371f1a-718b-4c72-9bbd-bed5202627ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 1, 'elapsed_time': 3.16, 'unit_time': 3.16, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 2, 'elapsed_time': 5.72, 'unit_time': 2.86, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 3, 'elapsed_time': 6.95, 'unit_time': 2.32, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 4, 'elapsed_time': 10.14, 'unit_time': 2.54, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 5, 'elapsed_time': 9.71, 'unit_time': 1.94, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 7, 'elapsed_time': 14.98, 'unit_time': 2.14, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 10, 'elapsed_time': 19.05, 'unit_time': 1.91, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 12, 'elapsed_time': 22.1, 'unit_time': 1.84, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 15, 'elapsed_time': 26.19, 'unit_time': 1.75, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 17, 'elapsed_time': 29.17, 'unit_time': 1.72, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 20, 'elapsed_time': 34.34, 'unit_time': 1.72, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 25, 'elapsed_time': 3.44, 'unit_time': 0.14, 'success': False}\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(50).collect()]\n",
    "\n",
    "def batch_size_optimiser():\n",
    "    batch_sizes = [1, 2, 3, 4, 5, 7, 10, 12, 15, 17, 20, 25, 30]\n",
    "    success = True\n",
    "    for size in batch_sizes:\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            batch_generate(batch_prompts=sample_instructions[:size])\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            success = False\n",
    "            break\n",
    "        finally:\n",
    "            elapsed = round(time.perf_counter() - start, 2)\n",
    "            unit_time = round(elapsed/size, 2)\n",
    "            yield {\"batch_size\": size, \"elapsed_time\": elapsed, \"unit_time\": unit_time, \"success\": success}\n",
    "\n",
    "for result in batch_size_optimiser():\n",
    "    print(\"- - \" * 10)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc57e4e-32bc-4a81-9a23-5bf57f9bba14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Select Batch Size\n",
    "\n",
    "Doesn't necessarily has to be the largest batch size that succeeded without running into an OOM error. It is probably better to choose the 2nd or 3rd largest successful batch size so that OOM errors can be reduced during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731ca081-ddb5-4d1d-8f12-0539c3c79871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the bactch size with minimum unit time\n",
    "OPTIMAL_BATCH_SIZE = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f498bba5-156e-4bd3-b01c-ce3e17ac54c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Inference Logic\n",
    "\n",
    "All of the generation logic is carried into a Pandas UDF. This helps with the set up on the workers. An iterator to interator architecture is followed to handle batching processes. \n",
    "\n",
    "In the case that the model runs into an OOM Error, the function handles the exception by returnin a OOM string as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be596558-8558-437a-a62d-e75507dbc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/functions.py:404: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "\n",
    "# Build Inference Function\n",
    "@SF.pandas_udf(\"string\", SF.PandasUDFType.SCALAR_ITER)\n",
    "def run_distributed_inference(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "\n",
    "    # External Imports\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Params\n",
    "    random_seed = 42\n",
    "\n",
    "    # Random seed set\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Load model to GPU\n",
    "    model.to(\"cuda:0\")\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    for prompts in iterator:\n",
    "        prompts = prompts.to_list()\n",
    "        try:\n",
    "            output = batch_generate(\n",
    "                batch_prompts=prompts, \n",
    "                tokenizer=tokenizer, \n",
    "                model=model\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # If out of memory, return a series of OOM strings that has the lenght of the input\n",
    "            output = [\"OOM\"] * len(prompts)\n",
    "\n",
    "        yield pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc8047e-87ab-41f4-851d-9df089d9d9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inference Configurations\n",
    "\n",
    "Automatically undertands how many workers are available in the cluster, and adjusts partitions accordingly. This means that the setup portion of the Pandas UDF which loads the model and tokenizer gets run only once during inference, and the data processing is handled with the iterator.\n",
    "\n",
    "Max Records Per Batch configuration controls how big the batch sizes are going to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80654ef-411d-490a-b683-aa605cdb205a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Auto get number of workers\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Subtract 1 to exclude the driver\n",
    "num_workers = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1  \n",
    "\n",
    "# Set the batch size for the Pandas UDF\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", OPTIMAL_BATCH_SIZE * 2)\n",
    "\n",
    "# Repartition\n",
    "articles_df = articles_df.repartition(num_workers)\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a037004-60c7-4e64-aab0-9165ff0c5c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run Distributed Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9c354e-24dd-48a8-be54-c96c453cf828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Apply Inference UDF\n",
    "articles_df = (\n",
    "    articles_df\n",
    "    .withColumn(\"llm_summary\", run_distributed_inference(SF.col(\"instruction\")))\n",
    "\n",
    ")\n",
    "\n",
    "# Materilize and Execute\n",
    "inference_start_time = time.perf_counter()\n",
    "articles_df.cache()\n",
    "articles_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "inference_elapsed_time = round(time.perf_counter() - inference_start_time, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8c6d6f-dd6a-40be-8a84-6eced296c343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean Summaries\n",
    "\n",
    "The summaries generated by the model also contains the prompt. This function cleanes the summary by removing the prompt, leaving only the generated text behind. The LLAMA V2 model uses a specific token to mark the ending of the instruction, which is used here to split the string. Updating the UDF while using a different model might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb7e010-00d5-49f4-ae27-69db859527d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# UDF Build\n",
    "clean_llm_summary = SF.udf(lambda x: x.split(\"[/INST]\")[-1].strip(), \"string\")\n",
    "\n",
    "# Apply UDF\n",
    "articles_df = (\n",
    "    articles_df.withColumn(\n",
    "        \"cleaned_llm_summary\", clean_llm_summary(SF.col(\"llm_summary\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cb8325-721c-44ae-bca1-ff5db0704925",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculate Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634f432c-e359-4729-8821-3c0141971cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@SF.udf(\"int\")\n",
    "def calculate_n_tokens(target_text):\n",
    "    return len(\n",
    "        tokenizer.encode_plus(\n",
    "            target_text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=True,\n",
    "            max_length=MAX_TOKENS,\n",
    "        )[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate article tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"article_token_count\", calculate_n_tokens(SF.col(\"article\"))\n",
    ")\n",
    "\n",
    "# Calculate instruction tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction_token_count\", calculate_n_tokens(SF.col(\"instruction\"))\n",
    ")\n",
    "\n",
    "# Calculate generated tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"generated_token_count\", calculate_n_tokens(SF.col(\"cleaned_llm_summary\"))\n",
    ")\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec175f1-54a4-4d95-b7e8-1cc8f0f65520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Display Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c0b40d-6f33-4c55-8689-607203b9924d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input ---\nTotal Article Count: 10000\nArticles Token Count: 9952025\nWith Instructions Token Count: 10934783\n\n--- Output ---\nGenerated Tokens Count: 3013560\nInference Elapsed Seconds: 9583.6114\nInference Elapsed Time: 2:39:43.611400\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import datetime\n",
    "\n",
    "text_stats = (\n",
    "    articles_df.groupBy()\n",
    "    .agg(\n",
    "        SF.count(SF.col(\"id\")).alias(\"articles_count\"),\n",
    "        SF.sum(SF.col(\"article_token_count\")).alias(\"total_article_tokens\"),\n",
    "        SF.sum(SF.col(\"instruction_token_count\")).alias(\"total_instruction_tokens\"),\n",
    "        SF.sum(SF.col(\"generated_token_count\")).alias(\"total_generated_tokens\"),\n",
    "    )\n",
    "    .first()\n",
    ")\n",
    "\n",
    "human_elapsed_time = str(datetime.timedelta(seconds=inference_elapsed_time))\n",
    "\n",
    "print(\"-\" * 3 + \" Input \" + \"-\" * 3)\n",
    "print(f\"Total Article Count: {text_stats['articles_count']}\")\n",
    "print(f\"Articles Token Count: {text_stats['total_article_tokens']}\")\n",
    "print(f\"With Instructions Token Count: {text_stats['total_instruction_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 3 + \" Output \" + \"-\" * 3)\n",
    "print(f\"Generated Tokens Count: {text_stats['total_generated_tokens']}\")\n",
    "print(f\"Inference Elapsed Seconds: {inference_elapsed_time}\")\n",
    "print(f\"Inference Elapsed Time: {human_elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a855bc-704c-4432-80ec-f95b3a64450d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c2ca3a-86fd-47dc-a7a7-94b20ffb070e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Table\n",
    "articles_df.write.mode(\"overwrite\").saveAsTable(\"llm_batch_inference_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2750819d-9d28-4ac4-9078-261cb6016a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1829729213475489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformer-batch-inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
