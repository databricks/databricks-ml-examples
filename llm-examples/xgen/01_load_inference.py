# Databricks notebook source
# MAGIC %md
# MAGIC # XGen-7B-8K-Base Inference on Databricks
# MAGIC
# MAGIC [XGen-7B-8K-Base](https://huggingface.co/Salesforce/xgen-7b-8k-base) is a 7B parameter model with context length of 8000.
# MAGIC
# MAGIC Environment for this notebook:
# MAGIC - Runtime: 13.2 GPU ML Runtime
# MAGIC - Instance: `g5.4xlarge` on AWS

# COMMAND ----------

# MAGIC %md
# MAGIC ## Install required packages
# MAGIC
# MAGIC * XGen-7B's tokenizer requires the `tiktoken` package.
# MAGIC * Upgrade `xformers` to accelerate training with memory-efficient attention.

# COMMAND ----------

# MAGIC %pip install -q tiktoken==0.4.0
# MAGIC %pip install -q -U xformers==0.0.20

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Inference

# COMMAND ----------

from transformers import AutoTokenizer
import transformers
import torch

model_name = "Salesforce/xgen-7b-8k-base"
pinned_revision = "3987e094377fae577bba039af1b300ee8086f9e1"
target_device = 0 if torch.cuda.is_available() else -1

tokenizer = AutoTokenizer.from_pretrained(
  model_name,
  trust_remote_code=True,
  revision=pinned_revision,
)
tokenizer.pad_token_id = tokenizer.eos_token_id  # Pad token must be set for batch inference.
pipeline = transformers.pipeline(
  "text-generation",
  model=model_name,
  tokenizer=tokenizer,
  torch_dtype=torch.bfloat16,
  trust_remote_code=False,
  device=target_device,
  revision=pinned_revision,
)

# COMMAND ----------

def generate(prompts, **kwargs):
  if "batch_size" not in kwargs:
    kwargs["batch_size"] = 1

  if "max_new_tokens" not in kwargs:
    kwargs["max_new_tokens"] = 512

  kwargs["do_sample"] = True

  outputs = pipeline(prompts, **kwargs)
  return [out[0]["generated_text"] for out in outputs]

# COMMAND ----------

# MAGIC %md
# MAGIC ### Inference on a single input

# COMMAND ----------

results = generate(["Write me travel blog to Colorado in fall"])
print(results[0])

# COMMAND ----------

# MAGIC %md
# MAGIC ### Batch inference

# COMMAND ----------

# From databricks-dolly-15k
inputs = [
  "Think of some family rules to promote a healthy family relationship",
  "In the series A Song of Ice and Fire, who is the founder of House Karstark?",
  "which weighs more, cold or hot water?",
  "Write a short paragraph about why you should not have both a pet cat and a pet bird.",
  "Is beauty objective or subjective?",
  "What is SVM?",
  "What is the current capital of Japan?",
  "Name 10 colors",
  "How should I invest my money?",
  "What are some ways to improve the value of your home?",
  "What does fasting mean?",
  "What is cloud computing in simple terms?",
  "What is the meaning of life?",
  "What is Linux?",
  "Why do people like gardening?",
  "What makes for a good photograph?",
]

# COMMAND ----------

# Set batch size
results = generate(inputs, batch_size=8)

for output in results:
  print(output)
  print('\n')

# COMMAND ----------

# MAGIC %md
# MAGIC ## Measure inference speed
# MAGIC
# MAGIC Text generation speed is often measured with token/s, which is the average number of tokens that are generated by the model per second.

# COMMAND ----------

import time

def get_num_tokens(text):
  inputs = tokenizer(text, return_tensors="pt").input_ids
  if torch.cuda.is_available():
    inputs = inputs.to("cuda")
  return inputs.shape[1]

def generate_and_measure_throughput(prompt, **kwargs):
  """
  Return tuple (number of tokens / sec, num tokens, output) of the generated tokens
  """
  if "max_new_tokens" not in kwargs:
    kwargs["max_new_tokens"] = 512

  kwargs.update(
    {
      "do_sample": True,
      # Make the pipeline return token ids instead of decoded text to get the number of generated tokens.
      "return_tensors": True,
    }
  )

  num_input_tokens = get_num_tokens(prompt)

  # Measure the time it takes for text generation.
  start = time.time()
  outputs = pipeline(prompt, **kwargs)
  duration = time.time() - start

  generated_token_ids = outputs[0][0]["generated_token_ids"]
  num_output_tokens = len(generated_token_ids)

  result = tokenizer.batch_decode(
    # [generated_token_ids[num_input_tokens:]],
    [generated_token_ids],
    skip_special_tokens=True,
  )
  result = "".join(result)

  return (num_output_tokens / duration, num_output_tokens, result)

# COMMAND ----------

throughput, n_tokens, result = generate_and_measure_throughput(["Write me travel blog to Colorado in fall"])
print(f"Result ({throughput} tokens/s, {n_tokens} tokens): {result}")
